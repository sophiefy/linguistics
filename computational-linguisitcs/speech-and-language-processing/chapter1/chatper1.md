# Regular Expressions, Text Normalization, Edit Distance

文本规范化（text normalization）指将原始文本转换为更方便处理的标准形式。

- 分词（tokenization）：将文本切分为更小的单位。
- 词形还原（lemmatization）：获取词的词根（root）。
- 词干提取（stemming）：可以认为是简化的词形还原。去掉词的后缀得到词干（stem）。
- 分句（sentence segmentation）：使用标点符号等线索将文本切分为句子。

## 正则表达式

正则表达式（regular expressions, regex）是一种用于在语料库（corpus）中搜索某种模式（pattern）的特制语言。

## 词

- 语料库（corpus）指一个计算机可读的文本或语音的集合。
- 词素（lemma）是一组有着相同词根、主词性和词义的词汇形式（lexical form）。

在处理实际的任务时，通常不会直接把词作为文本的内部单元，而是会将输入字符串切成token。Token可以是词，也可以是词的一部分。

## 语料库

由于语言受情景影响很大，所以在从语料库中构建计算模型是需要是谁，在什么语境，以什么目的建立了这个语料库。

## 分词

分词算法主要有两类。

- 自上而下：基于一定的标准和规则，如正则表达式。
- 自下而上：基于统计将词分为子词token，如字节对编码（byte-pair encoding, BPE）。

通常，在中文任务上不会用词为单位，而是字符。这是因为一方面汉字本身具有一定的语义，另一方面中文的词汇量太大。但对于日语而言，字符的粒度又太小，从而需要分词（word segmentation）。

### 字节对编码

BPE常用于大语言模型中。在BPE算法中，我们不会将词作为token，而是利用数据自动得到token的集合。

- 以所有的字符初始化词汇表。
- 在语料库中，寻找邻接频次最高的两个字符合并为一个新的token。
- 执行K次后，得到的词汇表包含初始的字符加上K个新的token。

注意该算法通常在词内部（inside words）中执行，而不会越过词边界。这就需要训练语料库的文本已经被切分为一些列词。

最后很可能是BPE得到的token覆盖了大部分的词，只有比较罕见的词会用多个token表示。

## 词规范化，词形还原和词干提取

- 词规范化（word normalization）：是将词或token转换为标准格式。最常见的大小写折叠（case folding），即将文本同一转换为小写。注意使用BPE等算法的系统中一般不会再做词规范化。
- 词形还原：可以让两个在形态上不同的形式有着相似的行为。例如网页搜索中，用户输入woodchucks应该也可以得到woodchuck有关的网页。

- 词干提取：主要是将单词后缀去掉，是一种最简单的形态分析。

## 分句

通常使用标点符号将文本切分为句子。但类似Inc.中的点并不能作为切分符，因此需要更精细的规则。

## 最小编辑距离

